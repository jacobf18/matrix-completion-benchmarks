<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>Matrix Completion Benchmarks Hub</title>
    <meta
      name="description"
      content="Lightweight dashboard for matrix completion benchmarks, including noisy synthetic, Hankel, tabular downstream metrics, and NNM benchmark catalog."
    />
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link
      href="https://fonts.googleapis.com/css2?family=Space+Grotesk:wght@400;500;700&family=IBM+Plex+Mono:wght@400;600&display=swap"
      rel="stylesheet"
    />
    <link rel="stylesheet" href="./styles.css" />
  </head>
  <body>
    <header class="hero">
      <div class="hero__bg"></div>
      <div class="container hero__content">
        <p class="eyebrow">Matrix Completion Benchmarks</p>
        <h1>Matrix Completion Benchmark Hub</h1>
        <p class="lede">
          Includes synthetic/noisy benchmarks, Hankel-structured recovery, tabular missing-data downstream
          evaluation, and a nuclear-norm benchmark index.
        </p>
        <div class="hero__cta">
          <a class="btn btn--solid" href="#explorer">Open Result Explorer</a>
          <a class="btn btn--ghost" href="#quickstart">Quickstart</a>
        </div>
      </div>
    </header>

    <main class="container stack">
      <section class="panel" id="quickstart">
        <h2>Quickstart</h2>
        <p>1) Synthetic noise sweep (`global_mean` + `soft_impute`):</p>
        <pre><code>PYTHONPATH=src python scripts/run_synthetic_noise_sweep.py \
  --noise-levels 0.05,0.1,0.2,0.35,0.5 \
  --algorithms global_mean soft_impute</code></pre>
        <p>2) Hankel benchmark presets:</p>
        <pre><code>PYTHONPATH=src python scripts/run_hankel_benchmarks.py \
  --preset-ids hankel_gaussian_sigma_0p01 hankel_gaussian_sigma_0p05 hankel_gaussian_sigma_0p10 \
  --algorithms global_mean soft_impute cadzow \
  --output-dir benchmarks/reports/hankel</code></pre>
        <p>3) Tabular downstream + multiple imputation:</p>
        <pre><code>PYTHONPATH=src python scripts/prepare_tabular_benchmark.py \
  --input-matrix /path/to/tabular_full.npy \
  --output-dataset-dir benchmarks/datasets/tabular_demo \
  --target-col 0</code></pre>
      </section>

      <section class="grid">
        <article class="panel">
          <h3>Benchmark Tracks</h3>
          <ul class="list">
            <li>Recommendation + causal panel benchmarks</li>
            <li>Synthetic noisy matrix completion sweeps</li>
            <li>Hankel-structured matrix completion</li>
            <li>Tabular imputation with downstream tasks</li>
          </ul>
        </article>
        <article class="panel">
          <h3>Key Methods</h3>
          <ul class="list">
            <li><code>soft_impute</code>, <code>nuclear_norm_minimization</code></li>
            <li><code>hyperimpute</code>, <code>missforest</code>, <code>forest_diffusion</code></li>
            <li><code>cadzow</code> (Hankel structured baseline)</li>
          </ul>
        </article>
        <article class="panel">
          <h3>NNM Catalog</h3>
          <p class="hint">Nuclear norm benchmark index:</p>
          <pre><code>benchmarks/nnm_catalog.yaml</code></pre>
          <p class="hint">Tracks access status + NNM evidence links.</p>
        </article>
      </section>

      <section class="panel">
        <h2>Plot Scripts</h2>
        <p>Noise sweep plot:</p>
        <pre><code>PYTHONPATH=src python scripts/plot_noise_sweep.py \
  --results-csv benchmarks/reports/noise_sweep/noise_sweep_results.csv \
  --metric nrmse \
  --output-path benchmarks/reports/noise_sweep/nrmse_vs_noise.png</code></pre>
        <p>Hankel benchmark plot:</p>
        <pre><code>PYTHONPATH=src python scripts/plot_hankel_results.py \
  --results-csv benchmarks/reports/hankel/hankel_results.csv \
  --metric nrmse_missing \
  --x-axis noise_sigma \
  --output-path benchmarks/reports/hankel/nrmse_missing_vs_noise.png</code></pre>
      </section>

      <section class="panel" id="explorer">
        <h2>Result Explorer (CSV)</h2>
        <p class="hint">
          Upload CSV results (for example
          <code>noise_sweep_results.csv</code> or <code>hankel_results.csv</code>), then choose x-axis and metric.
        </p>
        <div class="controls">
          <label class="control">
            <span>Results CSV</span>
            <input id="csvFile" type="file" accept=".csv,text/csv" />
          </label>
          <button id="loadDemoCsvBtn" class="btn btn--ghost btn--small" type="button">Load Demo CSV</button>
          <label class="control">
            <span>X-axis</span>
            <select id="xAxisSelect"></select>
          </label>
          <label class="control">
            <span>Metric</span>
            <select id="metricSelect"></select>
          </label>
          <label class="control">
            <span>Pattern</span>
            <select id="patternSelect"></select>
          </label>
          <label class="control">
            <span>Series View</span>
            <select id="seriesViewSelect">
              <option value="summary" selected>Mean line</option>
              <option value="raw">Raw per seed</option>
            </select>
          </label>
          <label class="control">
            <span>Error Bars</span>
            <select id="errorBarSelect">
              <option value="sd" selected>Standard deviation</option>
              <option value="ci95">95% confidence interval</option>
            </select>
          </label>
        </div>
        <div class="chart-wrap">
          <svg id="chart" viewBox="0 0 900 380" role="img" aria-label="Benchmark result line chart"></svg>
        </div>
        <div id="legend" class="legend"></div>
        <div class="table-wrap">
          <table id="resultsTable">
            <thead>
              <tr>
                <th>Series</th>
                <th>ID</th>
                <th>X</th>
                <th>Metric</th>
              </tr>
            </thead>
            <tbody></tbody>
          </table>
        </div>
      </section>

      <section class="panel" id="tabular-explorer">
        <h2>Tabular Eval Explorer (JSON)</h2>
        <p class="hint">
          Upload one or more tabular evaluation JSON files (for example
          <code>soft_impute_eval.json</code>, <code>mi_gaussian_eval.json</code>) to compare downstream metrics.
        </p>
        <div class="controls">
          <label class="control">
            <span>Evaluation JSON files</span>
            <input id="jsonFiles" type="file" accept=".json,application/json" multiple />
          </label>
          <button id="loadDemoJsonBtn" class="btn btn--ghost btn--small" type="button">Load Demo JSON</button>
          <label class="control">
            <span>Downstream Metric</span>
            <select id="jsonMetricSelect"></select>
          </label>
        </div>
        <p id="demoStatus" class="hint"></p>
        <div class="chart-wrap">
          <svg id="jsonChart" viewBox="0 0 900 340" role="img" aria-label="Tabular evaluation bar chart"></svg>
        </div>
        <div class="table-wrap">
          <table id="jsonResultsTable">
            <thead>
              <tr>
                <th>Method</th>
                <th>Task</th>
                <th>Target Col</th>
                <th>Metric</th>
                <th>Value</th>
              </tr>
            </thead>
            <tbody></tbody>
          </table>
        </div>
      </section>
    </main>

    <footer class="footer">
      <div class="container">
        <p>
          Static site for <code>matrix-completion-benchmarks</code>. Serve locally with
          <code>python -m http.server -d website 8080</code>.
        </p>
      </div>
    </footer>

    <script src="./app.js"></script>
  </body>
</html>
